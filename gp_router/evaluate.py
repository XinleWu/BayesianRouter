import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.nn.parallel import DistributedDataParallel as DDP
from peft import PeftModel
from math_verify import parse, verify
from policy_model import setup_lora_model, ResponseGenerator


# class DatasetManager:
#     def __init__(self):
#
#     def get_test_data
#
#
# def load_policy_for_inference(saved_dir: str,
#                               base_model_name_or_path: str,
#                               device: str = "cuda:0",
#                               dtype=torch.float16,
#                               adapter_name: str = "policy"):
#     # 1) æ‰¾åˆ° adapter å­ç›®å½•ï¼ˆPEFT é€šå¸¸æ”¾åœ¨ saved_dir/policyï¼‰
#     adapter_path = saved_dir
#     maybe_policy = os.path.join(saved_dir, "policy")
#     if os.path.isdir(maybe_policy):
#         adapter_path = maybe_policy
#
#     # 2) åŠ è½½ tokenizerï¼ˆä» saved_dirï¼‰
#     tokenizer = AutoTokenizer.from_pretrained(saved_dir)
#     tokenizer.pad_token = tokenizer.eos_token
#     tokenizer.padding_side = "left"  # decoder-only å¿…é¡»å·¦å¡«å……
#
#     # 3) åŠ è½½ base modelï¼ˆä¼˜å…ˆå°è¯• device_map="auto" + æŒ‡å®š dtypeï¼‰
#     base_model = AutoModelForCausalLM.from_pretrained(
#         base_model_name_or_path,
#         torch_dtype=dtype,
#         device_map={
#             "": 0
#         }  # å¦‚æœæœºå™¨ä¸Šæœ‰åŠ é€Ÿåº“ï¼Œä¼šè‡ªåŠ¨æŠŠæ¨¡å‹æ”¾åˆ°åˆé€‚è®¾å¤‡
#     )
#
#     # 4) æŠŠ adapter (LoRA) åº”ç”¨åˆ° base
#     model = PeftModel.from_pretrained(base_model, adapter_path, adapter_name=adapter_name, torch_dtype=dtype)
#     model.set_adapter(adapter_name)
#     model.eval()
#
#     # å¦‚æœ device_map="auto" æˆåŠŸï¼Œmodel å·²ç»åˆ†é…å¥½ï¼›å¦åˆ™æŠŠæ•´ä¸ª model ç§»åˆ° device
#     try:
#         model.to(device)
#     except Exception:
#         pass
#
#     return model, tokenizer
#
#
# def generate_response_test(response_generator, dataset_manager, out_path,
#                            dataset_name: str = 'mt-bench',
#                            batch_size: int = 16,
#                            max_new_tokens: int = 256,
#                            chunk_size: int = 4):
#     # unwrap DDP if needed (so generate works)
#     orig_model = getattr(response_generator, "model", None)
#     unwrapped = False
#     try:
#         if isinstance(orig_model, DDP):
#             response_generator.model = orig_model.module
#             unwrapped = True
#
#         test_set = dataset_manager.get_test_data(dataset_name)
#         n = len(test_set)
#         print(f'len test: {n}')
#
#         with open(out_path, "w", encoding="utf-8") as fout:
#             for i in range(0, n, batch_size):
#                 batch = test_set[i:i + batch_size]
#                 queries = []
#                 for ex in batch:
#                     if "question" in ex:
#                         queries.append(ex["question"])
#
#                 prompts = [response_generator.generate_prompt(q) for q in queries]
#                 outputs = response_generator.generate_responses(
#                     batch=queries,
#                     n_responses=1,
#                     prompts=prompts,
#                     override_temperature=0.0,
#                     do_sample_override=False,
#                     max_new_tokens=max_new_tokens,
#                     chunk_size=chunk_size,
#                 )
#
#                 for (q_out, p_out, resp), ex in zip(outputs, batch):
#                     result = {
#                         "instruction": q_out,
#                         "output": resp
#                     }
#                     fout.write(json.dumps(result, ensure_ascii=False) + "\n")
#     finally:
#         if unwrapped:
#             response_generator.model = orig_model
#
#
# saved_dir = "/data/cs.aau.dk/zh45qz/router_data/output_policy/both"  # ä½ ä¿å­˜çš„ç›®å½•ï¼ˆå« policy/ï¼‰
# base_model = "RLHFlow/LLaMA3-SFT"  # è®­ç»ƒæ—¶ç”¨çš„ base æ¨¡å‹åæˆ–æœ¬åœ°è·¯å¾„
# model, tokenizer = load_policy_for_inference(saved_dir, base_model, device="cuda:3", dtype=torch.float16)
# dataset_manager = DatasetManager("HuggingFaceH4/mt_bench_prompts")
#
# response_generator = ResponseGenerator(model, tokenizer)
# generate_response_test(response_generator, dataset_manager, out_path='output_full/both.json')




import os
import json
from datasets import load_dataset
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from torch.nn.parallel import DistributedDataParallel as DDP

#### Dataset manager ####
class DatasetManager:
    def __init__(self, dataset_name: str = "HuggingFaceH4/mt_bench_prompts", split: str = "train"):
        self.dataset_name = dataset_name
        self.split = split
        ds = load_dataset(dataset_name)
        # å–æŒ‡å®š splitï¼ˆmt-bench_prompts åªæœ‰ train splitï¼‰
        self.dataset = ds[split]
        print(f'len data: {len(self.dataset)}')

    def get_test_data(self):
        # è¿”å›ä¸€ä¸ªå¯è¿­ä»£çš„ datasetï¼ˆhuggingface Datasetï¼‰
        return self.dataset

def load_policy_for_inference(saved_dir: str,
                              base_model_name_or_path: str,
                              device: str = "cuda:0",
                              dtype=torch.float16,
                              adapter_name: str = "policy",
                              use_adapter: bool = True):
    # tokenizer ä¸€å®šä» base æ¨¡å‹åŠ è½½
    tokenizer = AutoTokenizer.from_pretrained(base_model_name_or_path, use_fast=False)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    # load base model (try device_map="auto" first)
    try:
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name_or_path,
            torch_dtype=dtype,
            device_map="auto",
        )
    except Exception:
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name_or_path,
            torch_dtype=dtype,
        )
        try:
            base_model.to(device)
        except Exception:
            pass

    if use_adapter:
        adapter_path = saved_dir
        maybe_policy = os.path.join(saved_dir, "policy")
        if os.path.isdir(maybe_policy):
            adapter_path = maybe_policy

        model = PeftModel.from_pretrained(
            base_model,
            adapter_path,
            adapter_name=adapter_name,
            torch_dtype=dtype,
        )
        # ensure adapter set (PeftModel should have this)
        try:
            model.set_adapter(adapter_name)
        except Exception:
            pass
        # try move to device if not already placed
        try:
            model.to(device)
        except Exception:
            pass
    else:
        model = base_model
        try:
            model.to(device)
        except Exception:
            pass

    model.eval()
    return model, tokenizer



#### è¾…åŠ©ï¼šä»ä»»æ„åµŒå¥—è¿”å›ç»“æ„ä¸­æ‰¾ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆç”¨äºæå–ç”Ÿæˆç»“æœï¼‰ ####
def find_first_string(x):
    if x is None:
        return None
    if isinstance(x, str):
        return x
    if isinstance(x, dict):
        for v in x.values():
            s = find_first_string(v)
            if s:
                return s
    if isinstance(x, (list, tuple)):
        for v in x:
            s = find_first_string(v)
            if s:
                return s
    return None

#### è¾…åŠ©ï¼šæŠŠ history + å½“å‰ user_turn è½¬ä¸º prompt å­—ç¬¦ä¸²ï¼ˆç®€å• role æ ‡è®°ï¼‰ ####
def build_prompt_from_history(history, user_turn):
    # history: list of {"role": "user"/"assistant", "content": "..."}
    parts = []
    for m in history:
        role = m.get("role", "user")
        txt = m.get("content", "")
        # ç®€å• role prefixï¼ˆä½ å¯ä»¥æ”¹æˆæ¨¡å‹è®­ç»ƒæ—¶ç”¨çš„æ ¼å¼ï¼‰
        prefix = "User:" if role == "user" else "Assistant:"
        parts.append(f"{prefix} {txt}")
    parts.append(f"User: {user_turn}")
    # åŠ ä¸Š assistant çš„ç©ºä½ï¼Œè®©æ¨¡å‹ä»¥ assistant çš„è§’è‰²å›å¤ï¼ˆæœ‰äº› prompt template ä¼šéœ€è¦ï¼‰
    parts.append("Assistant:")
    return "\n".join(parts)

#### ä¸»æµç¨‹ï¼šé’ˆå¯¹æ¯æ¡ mt-bench prompt è¿›è¡Œå¤šè½®ç”Ÿæˆå¹¶å†™å‡º JSONL ####
def generate_response_test(response_generator, dataset_manager, out_path,
                           dataset_name: str = 'mt-bench',
                           batch_size: int = 16,
                           max_new_tokens: int = 256,
                           chunk_size: int = 4):
    orig_model = getattr(response_generator, "model", None)
    unwrapped = False
    try:
        if isinstance(orig_model, DDP):
            response_generator.model = orig_model.module
            unwrapped = True

        test_set = dataset_manager.get_test_data()
        n = len(test_set)
        print(f'len test: {n}')

        with open(out_path, "w", encoding="utf-8") as fout:
            for i in range(0, n, batch_size):
                batch = test_set[i:i + batch_size]

                for ex_idx, ex in enumerate(batch):
                    prompts = ex["prompt"]  # MT-Bench å¤šè½® prompt
                    conversation = []
                    history_texts = []  # ç”¨äºæ„å»ºå¤šè½®ä¸Šä¸‹æ–‡

                    for user_turn in prompts:
                        queries = [user_turn]

                        # æ„å»º prompt æ—¶åŠ å…¥ history_texts
                        prompts_for_model = []
                        for q in queries:
                            # æŠŠ history æ‹¼æ¥åˆ°å½“å‰ user_turn
                            if history_texts:
                                prompt_text = "\n".join(history_texts + [q])
                            else:
                                prompt_text = q
                            prompts_for_model.append(prompt_text)

                        outputs = response_generator.generate_responses(
                            batch=queries,
                            n_responses=1,
                            max_input_length=400,
                            prompts=prompts_for_model,
                            override_temperature=0.0,
                            do_sample_override=False,
                            max_new_tokens=max_new_tokens,
                            chunk_size=chunk_size,
                        )

                        # ä¿ç•™åŸæ¥çš„è§£åŒ…æ–¹å¼
                        _, _, resp_text = outputs

                        conversation.append({"role": "user", "content": user_turn})
                        conversation.append({"role": "assistant", "content": resp_text})
                        history_texts.append(user_turn)
                        history_texts.append(resp_text)

                    # ä¿å­˜æ•´æ¡å¤šè½®å¯¹è¯
                    result = {
                        "id": ex_idx + i,
                        "conversation": conversation
                    }
                    fout.write(json.dumps(result, ensure_ascii=False) + "\n")

    finally:
        if unwrapped:
            response_generator.model = orig_model


#### ç”¨æ³•ç¤ºä¾‹ï¼ˆæ›¿æ¢ä¸ºä½ è‡ªå·±çš„è·¯å¾„ / model / ResponseGeneratorï¼‰ ####
if __name__ == "__main__":
    saved_dir = "/data/cs.aau.dk/zh45qz/router_data/output_policy/both"
    base_model = "RLHFlow/LLaMA3-SFT"
    device = "cuda:2"

    # model, tokenizer = load_policy_for_inference(saved_dir, base_model, device=device, dtype=torch.float16)

    model, tokenizer = load_policy_for_inference(
        saved_dir=saved_dir,  # è·¯å¾„æ— æ‰€è°“ï¼Œä¸ä¼šç”¨åˆ°
        base_model_name_or_path="RLHFlow/LLaMA3-SFT",
        device="cuda:2",
        dtype=torch.float16,
        use_adapter=True  # ğŸš¨ å…³é”®ï¼šç¦ç”¨ adapter
    )

    dataset_manager = DatasetManager("lmarena-ai/arena-hard-auto", split="train")

    # ä½ çš„ ResponseGenerator ç±»ï¼šæ„é€ æ—¶ä¼ æ¨¡å‹å’Œ tokenizerï¼ˆä¿æŒä¸ä½ ç°æœ‰å®ç°ä¸€è‡´ï¼‰
    response_generator = ResponseGenerator(model, tokenizer)

    generate_response_test(response_generator, dataset_manager, out_path="output_full/sft_mtbench.jsonl")

